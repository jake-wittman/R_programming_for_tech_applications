---
title: "Classifying penguins in R"
output: html_document
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
```

For these analyses we'll be using the `palmerpenguins` data provided in the package of the same name.
Read more about the palmerpenguins package [here](https://allisonhorst.github.io/palmerpenguins/[https://allisonhorst.github.io/palmerpenguins/).

Let's load the packages we'll be using. 

```{r}
# install.packages("palmerpenguins")
library(palmerpenguins)
library(ggplot2)
library(GGally) # Used for pairwise correlation plots
library(dplyr)
library(MASS) # For the lda() function
library(purrr)
```

# Models used in this example

We'll be looking at two different types of classifier models in this example: logistic regression and linear discriminant analysis. Logistic regression is often used when you have a single response variable that has two possible outcomes (*e.g.* yes/no, alive/dead, 1/0). Logistic regression is also a commonly used modeling framework if you want to make inferences about the relationship between different predictors and your response variable (*e.g.* does an individuals blood-pressure have a statistically significant association with the occurrence of a heart attack?). 

Extensions of logistic regression can be used if your response variable has more than 2 possible outcomes, although in my experience these models are not used very often. There are a variety of models that can be used for predicting a response variable with >2 outcomes, but we will focus on linear discriminant analysis. Linear discriminant analysis is a relatively simple model framework that can be used to predict a response with several categories.

There are two datasets in this package: penguins and penguins_raw. The penguins dataset has already been pre-cleaned for us, so we will use that. You can load both datasets into your R environment with this function.

```{r}
data("penguins", package = "palmerpenguins")
```

First step: always take a peek at the data. Immediately, I notice some of the values are NA.
Also there are four numeric variables associated with each penguin species. These are all measurements of different aspects of the penguin body.

```{r}
# Check that each variable is coded correctly and see what they are
glimpse(penguins)
```

Let's see how many rows there are with missing observations. 
```{r}
# Total rows in dataset
nrow(penguins)
# Total rows with complete cases (i.e. no missing observations)
sum(complete.cases(penguins))
```
Looks like there are 11 cases with missing data. This is a useful check in any dataset. Many modeling methods cannot use observations where data are missing (although there are methods, such as multiple imputation, to deal with missing data) so this gives us an estimate of the actual sample size available for our model.

Next I want to examine plots of the data to identify and interesting trends, look for obvious patterns, or look for areas where we might run into trouble. One potential trouble spot if you're building linear models is correlation between predictors. If your model will be used for making inference (*i.e.* my variable x has a statistically significant association with the response variable y), correlation between two predictors can make this difficult. Correlation between predictors is slightly less important if the goal is just prediction. 

I like to use the `ggpairs()` function from the `GGally` pacakge to quickly generate pairwise plots of variables. If your variables are correctly coded as numeric, factor, integer etc. it automatically produces a number of useful plots for examining each variable individually and their pairwise combinations.

```{r, out.width='110%', out.height='120%'}
# To keep this graph manageable, we'll just look at pairs plots for the first
# six columns
ggpairs(penguins, columns = 1:6)
```

The plots along the diagonal are summaries of each individual variable, with the plot type dependent on the type of variable being plotted. The first two diagonal plots are bar graphs showing counts of observations in each category (so the upper left plot is the number of observations for each of the three species, and the next plot along the diagonal is the number of observations from each island). For the continuous numeric variables we're given density plots that show the distribution of observations for each variable. The plots off the diagonal give you information on how any two variables in this pairs plot are related. We also get information on the correlation if the two variables being used are both continuous.

In the bargraph of species counts on each island (second row, first column), its interesting to note that apparently all three species are not found on the same island together, and two of the species only appear on a single island. We can verify this and see which penguins are present on which islands with some summary functions.

```{r}
penguins %>% 
   group_by(species, island) %>% 
   summarise(species_count = n())
```
   
Also, we should note the correlation between bill length, bill depth, flipper length, and body mass apparent in the scatterplots. This will mean that these variables share a lot of predictive information and we will likely start to see diminishing returns in our model's accuracy as we include more and more of these correlated predictors. Correlated predictors also prove troublesome if you're trying to make inferences about the association between a predictor and a response.

It appears too that there are two distinct groups present on the bill depth vs flipper length and bill depth vs body mass plots. Let's color these plots by species to see if it is clear why these groups are present.

```{r}
ggplot(penguins,
       aes(x = bill_depth_mm,
           y = body_mass_g,
           colour = species,
           shape = species)) +
   geom_point() +
   theme_bw() +
   scale_color_manual(values = c("darkorange","purple","cyan4"))

ggplot(penguins,
       aes(x = bill_depth_mm,
           y = flipper_length_mm,
           colour = species,
           shape = species)) +
   geom_point() +
   theme_bw() +
   scale_color_manual(values = c("darkorange","purple","cyan4"))
```

Bill depth, flipper length, and body mass seem to be quite different for Gentoo penguins than for the other two penguins. That will be helpful later in the linear discriminant analysis when we try to predict penguin species from the data available.

Lets look at two more plots with bill length:

```{r}
ggplot(penguins,
       aes(x = bill_length_mm,
           y = bill_depth_mm,
           colour = species,
           shape = species)) +
   geom_point() +
   theme_minimal() +
   scale_color_manual(values = c("darkorange","purple","cyan4"))

ggplot(penguins,
       aes(x = bill_length_mm,
           y = flipper_length_mm,
           colour = species,
           shape = species)) +
   geom_point() +
   theme_minimal() +
   scale_color_manual(values = c("darkorange","purple","cyan4"))
```
   

These plots of bill length x bill depth and flipper length show that these three variables seem to do a good job at distinguishing all three of our species. 

# Logistic Regression

We can use logistic regression to predict a binary outcome, such as sex. Mathematically speaking, we are modeling the probability of our response variable conditional on our predictor variables $Pr(Y | X)$. Perhaps we want to know if we can predict the sex of an Adelie penguin just by measuring their flipper length. Logistic regression is a good tool for this. Because there are likely differences in how flipper length changes among male and female penguins of different species, we'll just use the most common species in this dataset, the Adelie penguin.

We'll remove any rows with NA values so we only have complete cases, filter the data to just Adelie penguins, and then split into a training, validation, and testing datasets.
```{r}
penguins <- penguins[complete.cases(penguins), ]
adelie_penguins <- filter(penguins, species == "Adelie")
```

Sample 60% of the dataset for the training

```{r}
# Set the seed so the random split is reproducible in the future.
set.seed(314)
train_ad_penguins <- sample_frac(adelie_penguins, 0.60)
```


Use anti-join to put the remaining 40% in another dataframe.

```{r}
# Call it not training since it doesn't contain the training subset
not_training_ad_penguins <- anti_join(adelie_penguins, train_ad_penguins)

# Further split the not_training dataset into a validation and testing set
# We're splitting the not training dataset in half, resulting in each being 20% of the original
validation_ad_penguins <- sample_frac(not_training_ad_penguins, 0.5)
test_ad_penguins <- anti_join(not_training_ad_penguins, validation_ad_penguins)
```



```{r}
# Logistic regression is a type of generalized linear regression, so we use the glm()
# function and specify family = "binomial"
log_reg1 <- glm(sex ~ flipper_length_mm, 
                data = train_ad_penguins,
                family = "binomial")
```

Normally I would want to check residual plots of this model to make sure the assumptions of the model are being met. However, interpreting residual plots for logistic regression is tricky. If you need to do it, I recommend the `DHARMa` package. I won't be going into how to use that package in this example.

Look at the summary and lets interpret these coefficients (although I should note that if the assumptions aren't met, the interpretation of statistical significance isn't valid).

```{r}
summary(log_reg1)
```

The intercept in this model doesn't have much meaning. In the majority of linear models fit with `lm()` or `glm()`, the intercept represents the expected value of the response when all of the predictors in the model are 0. The estimate for the intercept in a logistic regression is the log odds that any given penguin is male when their flipper length is 0. Of course we do not expect to encounter a penguin with flipper length 0, so in this model formulation the intercept does not have a meaningful biological interpretation. If you wanted a more interpretable intercept, you could center the predictor variable body mass at its mean. Then the intercept would be the log odds that a penguin with average body mass is male. However, if all we care about is prediction, the interpretation of the intercept isn't that important.

How do I know that the intercept is the log odds that a penguin is male given 0 flipper length? When R works with categorical variables in models it orders them alphanumerically and then recodes them. So if your variable sex takes on values of "male" and "female", "female" is recoded as 0 and "male" is recoded as 1. Logistic regression then models the probability that an observation has a response of 1 given the values of the predictor variable. We'll see in a minute how we can easily get predicted probabilities that a penguin is female.

The slope estimate for flipper length is interpreted as the log odds of being male increases by 0.102 for every 1 mm increase in flipper length. To get the output on a scale that is slightly easier to interpret, we can exponentiate the slope coefficient.


```{r}
exp(coef(log_reg1)[2])
```


This number is the odds ratio associated with an increase in 1 mm increase of flipper length.
An odds ratio > 1 means that as the predictor variable increases, the odds that the response variable is 1 increases. An odds ratio < 1 means that as the predictor variable decreases, the odds that the response variable is 1 decreases. Another way to interpret this coefficient is that as flipper length increases by 1 mm, the odds of being male increase by 11.8% (0.118 * 100 = 11.8%). These interpretations are kind of weird in this example, but are useful when modeling something like the whether or not someone will have a disease based on certain predictors.

Visualizing logistic regression is often one of the best ways to interpret your model. This is relatively easy to do with a single predictor. We'll set up sample data along the range of values we wish to predict. Looking at a summary for flipper length, 172 mm is the minimum and 231 mm is the maximum observed value in the overall data. Let's predict across that range + a little on either end.

```{r}
# Create a set simulated flipper length measurements along the range of our observed
# values. 
pred_dat <- data.frame(flipper_length_mm = seq(160, 240, length.out = 100))
# We specify type = "response" to get predictions on the probability scale.
# This gives us a vector of predicted probabilities based on our simulated flipper length
pred_values <- predict(log_reg1, newdata = pred_dat, type = "response")
```

Because our model coded female = 0 and male = 1, this is a visualization of the probability that a penguin is male given flipper length.
```{r}
pred_dat %>% 
   mutate(pred_values = pred_values) %>% # Add the predicted probabilites to the dataframe
   ggplot(aes(x = flipper_length_mm, y = pred_values)) +
   geom_line() +
   labs(y = "Probability a penguin is male")
```

A fairly straightforward transformation gives us the probability a penguin is female given body weight. We subtract the predicted probabilites from 1 to reverse the plot and find the probability that a penguin is female.

```{r}
pred_dat %>% 
   mutate(pred_values = (1 - pred_values)) %>% # Add the predicted probabilites to the dataframe
   ggplot(aes(x = flipper_length_mm, y = pred_values)) +
   geom_line() +
   labs(y = "Probability a penguin is female")
```
   

Lets plot both lines on this graph
```{r}
pred_dat %>% 
   mutate(pred_values_female = (1 - pred_values),
          pred_values_male = pred_values) %>% # Add the predicted probabilites to the dataframe
   ggplot(aes(x = flipper_length_mm)) +
   geom_line(aes(y = pred_values_female, colour = "purple")) +
   geom_line(aes(y = pred_values_male, colour = "darkorange")) +
   labs(y = "Probability") +
   scale_color_identity(name = "Sex",
                        breaks = c("purple", "darkorange"),
                        labels = c("Female", "Male"),
                        guide = "legend")
   
```

Looking at this graph, we can see that there will be an intermediate area of flipper length where our model won't be sure whether the penguin is male or female based on flipper length alone.

Lets use our validation dataset to see how our model performs. With logistic regression we have the ability to decide what cutoff point we use to determine if a penguin is male or female. For this example, we have no real reason to pick a cutoff point different from 0.50. However, if you were modeling something where the consequences of a false negative are worse than the consequences of a false positive, you may wish to use a different cutoff point.

```{r}
validation_preds <- predict(log_reg1, newdata = validation_ad_penguins, type = "response")
```

If the predicted probability is less than or equal to 0.5, the prediction is that the penguin is female, if it is greater than 0.5, the prediction is that the penguin is male.

```{r}
validation_classifier <- ifelse(validation_preds <= 0.5, "female", "male")
```

We can compare our model predictions to the actual observed penguin sexes. 
```{r}
sum(validation_classifier == validation_ad_penguins$sex) / nrow(validation_ad_penguins)
```
This classifier gets the estimate right about 72% of the time.

We might want to see more than just the success rate - lets look at false classifications to see where our model is going wrong. This is termed a confusion matrix in predictive modeling literature.

```{r}
# The argument dnn tells table to label our rows as the observed values and 
# columns as the predicted values
table(validation_ad_penguins$sex, validation_classifier,
      dnn = c("observed", "predicted"))
```

If you sum down the columns of this table, you have the total number of females predicted by our model in the left column and total number of males predicted in the right. The sum along the rows gives the total females in the actual validation data on the top and total males in the validation data on the bottom.

It might be more useful to look at these as percentages though.

```{r}
prop.table(table(validation_ad_penguins$sex, validation_classifier,
                 dnn = c("observed", "predicted")))
```


Our model misclassified females as males 13.7% of the time and misclassified males as females 13.7% of the time as well. It's possible we could improve our model predictions by optimizing the cut-off point but that's beyond the scope of this example. You could also improve this model by including more predictors. Let's try adding body mass to the model to see how the prediction changes. 

```{r}
# Logistic regression is a type of generalized linear regression, so we use the glm()
# function and specify family = "binomial"
log_reg2 <- glm(sex ~  body_mass_g + flipper_length_mm, 
                data = train_ad_penguins,
                family = "binomial")
summary(log_reg2)
```

```{r}
validation_preds2 <- predict(log_reg2, newdata = validation_ad_penguins)
validation_classifier2 <- ifelse(validation_preds2 <= 0.5, "female", "male")
sum(validation_classifier2 == validation_ad_penguins$sex) / nrow(validation_ad_penguins)
table(validation_ad_penguins$sex, validation_classifier2,
                 dnn = c("observed", "predicted"))
prop.table(table(validation_ad_penguins$sex, validation_classifier2,
                 dnn = c("observed", "predicted")))
```

We've managed to successfully classify every penguin in the validation set! Now, let's take a look at the test set.

```{r}
test_preds <- predict(log_reg2, newdata = test_ad_penguins)
test_classifier <- ifelse(test_preds <= 0.5, "female", "male")
sum(test_classifier == test_ad_penguins$sex) / nrow(test_ad_penguins)
table(test_ad_penguins$sex, test_classifier,
                 dnn = c("observed", "predicted"))
prop.table(table(test_ad_penguins$sex, test_classifier,
                 dnn = c("observed", "predicted")))
```

We didn't predict every penguin in the testing set correctly, but that's fine. You would not expect to correctly predict every test set since we did not fit the model to this data. We could try adding more variables but that might result in overfitting the data since we're only working with a subset of the whole penguins data. There are ways to assess overfitting in the validation set but again this outside the scope of this example. 

# Linear Discriminant Analysis

Lets build a classifier for the different penguin species. After looking at the data, I'm pretty confident we have some variables that will provide some good predictive/classification value for these penguin species.

The lda() function uses syntax similar to lm() or glm() if you're familiar with those.

Let's start with just bill length and see how this classifier performs.

First we need to create our training, validation, and test datasets.
```{r}
train_penguins <- sample_frac(penguins, 0.60)
```


Use anti-join to put the remaining 40% in another dataframe.

```{r}
# Call it not training since it doesn't contain the training subset
not_training_penguins <- anti_join(penguins, train_penguins)

# Further split the not_training dataset into a validation and testing set
# We're splitting the not training dataset in half, resulting in each being 20% of the original
validation_penguins <- sample_frac(not_training_penguins, 0.5)
test_penguins <- anti_join(not_training_penguins, validation_penguins)
```

Now we can fit our LDA.

```{r}
lda_train1 <- lda(species ~ bill_depth_mm + flipper_length_mm,
                  data = train_penguins)
lda_train1

```

The "prior probability" part of the output just tells us what proportion of our data belongs to each species category (*e.g.* the number of Adelie penguins divided by the total number of penguins in the data). LDA uses this information as part of it's decision rule to assign an observation to a particular class.

The group means give us a further breakdown based on the predictor variables. For a continuous variable, like bill depth of flipper length, it gives us the average bill depth or flipper length for each class of our response variable, in this case each penguin species.

The coefficients of linear discriminants are parameters the model uses to weight the predictor values of an observation and then assign that observation to a category. It's similar to how a linear regression works where the coefficient is multiplied times the value of the predictor to get an estimated response. Linear discriminant analysis works a bit differently in that it takes our predictor variables and transforms them into two new predictor variables, Linear Discriminant 1 and Linear Discriminant 2 (or LD1 and LD2), rather than using the raw predictor variables themselves. We can plot these new LD variables and will hopefully see that our points separate nicely into the categories we are trying to predict. You can do this by using the `plot()` function on the model object.

```{r}
plot(lda_train1)
```

This plot isn't very pretty, and is quite hard to read. It looks like our penguin species are being sorted into two rough clouds of points based on the newly calculated LD variables. Let's calculate the LD variables by hand and create our own version of the plot that is easier to read. The coefficients of linear discriminant values can be accessed in the model object `lda_train1$scaling`. 

```{r}
# View LD coefficients
lda_train1$scaling

# Each individual coefficient can be accessed using square brackets to index the matrix
# This code returns the LD coefficient from row 1 in column 2
lda_train1$scaling[1, 2]

# Use these coefficients to calculate LD1 and LD2 by hand
train_penguins <- train_penguins %>% 
   mutate(LD1_hand = lda_train1$scaling[1, 1] * bill_depth_mm + lda_train1$scaling[2, 1] * flipper_length_mm,
          LD2_hand = lda_train1$scaling[1, 2] * bill_depth_mm + lda_train1$scaling[2, 2] * flipper_length_mm)

# Produce a prettier plot
ggplot(train_penguins, aes(x = LD1_hand, y = LD2_hand, colour = species)) +
   geom_point() +
   theme_minimal() +
   scale_color_manual(values = c("darkorange","purple","cyan4"))
```

That's much nicer to look at, and easier to see what is going on as well. You might notice that the axes on this plot cover a different range than the axes on the plot we made with `plot(lda)`. This is because the LD values being plotted have been centered so the mean value of each LD variable is 0. We can access these centered LD values very easily and produce a plot that is almost identical to the one produced by `plot(lda)`.

```{r}
lda_viz <- predict(lda_train1) 
lda_viz <- cbind(train_penguins, lda_viz$x)
ggplot(lda_viz, aes(x = LD1, y = LD2, colour = species)) +
   geom_point() +
   theme_minimal() +
   scale_color_manual(values = c("darkorange","purple","cyan4"))
```

In this plot we see two clouds of points, one for the Gentoo penguins and another for the Adelie and Chinstrap penguins. It looks like these two predictor variables are useful for classifying Gentoo and non-Gentoo penguins, but will perform poorly in determining if a penguin is Adelie or Chinstrap.

Let's see how the model does on the validation set.
```{r}
lda_val1 <- predict(lda_train1, newdata = validation_penguins)
lda_val1$class
```

`lda_test1$class` tells us what class every observation in the validation set gets classified to.

Let's calculate the accuracy of our model on the validation set

```{r}
sum(lda_val1$class == validation_penguins$species) / nrow(validation_penguins)
```

89% isn't bad, especially considering how much the Adelie and Chinstrap penguins seemed to overlap in our LD plot.
Does the model consistently mispredict a certain species? Let's look at the confusion matrix

```{r}
table(validation_penguins$species, lda_val1$class,
      dnn = c("observed", "predicted"))
```

As we suspected, most of the confusion our model has is misclassyfing Adelie as Chinstrap and vice-versa. We can visualize these misclassifications by producing a plot of our model. The model uses decision boundaries when classifying our observations, that is a point is classified as a particular species depending on which side of the LD boundaries it falls into. We can see this with the code below.

```{r}
# To create our boundaries, we have to generate a grid of values for our 
# two predictor variables. We can do this with the expand.grid() function.

boundary_data <- expand.grid(bill_depth_mm = seq(min(penguins$bill_depth_mm),
                                                 max(penguins$bill_depth_mm),
                                                 length = 300),
                             flipper_length_mm = seq(min(penguins$flipper_length_mm),
                                                     max(penguins$flipper_length_mm),
                                                     length = 300))
# Now we use the boundary data with our grid of predictor variables to get predictions
# from the model
lda_boundaries <- data.frame(boundary_data,
                             species = as.numeric(predict(lda_train1, boundary_data)$class))

# Lastly, we'll make a scatter plot of flipper length by bill depth, color the points by species
# and then draw the decision boundaries with stat_contour
ggplot(validation_penguins,
              aes(x = bill_depth_mm,
                  y = flipper_length_mm,
                  color = species)) +
   geom_point() +
   stat_contour(data = lda_boundaries,
                aes(x = bill_depth_mm, y = flipper_length_mm, z = species),
                colour = "black") +
   scale_color_manual(values = c("darkorange","purple","cyan4"))
```

We can see that our Gentoo penguins all fall neatly one one side of one of a decision boundaries, but the Adelie and Chinstrap penguins do not. You should be able to identify the two Adelie that were misclassified as Chinstrap penguins in the middle decision region, and the five Chinstrap penguins that were misclassified as Adelie penguins in the bottom region.

Let's try and improve our model. I want to add bill length now as a predictor variable. 

```{r}
lda_train2 <- lda(species ~ bill_depth_mm + flipper_length_mm + bill_length_mm,
                  data = train_penguins)
lda_train2
```


Let's produce a plot of this model to see how our points are grouped. Once you get above 2 predictor values, it becomes a little more complicated to plot the decision boundaries, and we have to plot them on the plot of LD1 vs LD2 since it's hard to visualize the decision boundaries for a 3D scatterplot. For whatever reason, I was having difficulty plotting the decision boundaries themselves, so I made do with just plotting a bunch of points within each region and then coloring them by the species.

```{r}
# To create our boundaries, we have to generate a grid of values for our 
# two predictor variables. We can do this with the expand.grid() function.

boundary_data <- expand.grid(bill_depth_mm = seq(min(penguins$bill_depth_mm),
                                                 max(penguins$bill_depth_mm),
                                                 length = 15),
                             flipper_length_mm = seq(min(penguins$flipper_length_mm),
                                                     max(penguins$flipper_length_mm),
                                                     length = 15),
                             bill_length_mm = seq(min(penguins$bill_length_mm),
                                                  max(penguins$bill_length_mm),
                                                     length = 15))
# Now we use the boundary data with our grid of predictor variables to get predictions
# from the model
lda_boundaries <- predict(lda_train2, boundary_data)
lda_boundaries_df <- data.frame(LD1 = lda_boundaries$x[, 1],
                                LD2 = lda_boundaries$x[, 2],
                                species = lda_boundaries$class)
ggplot(lda_boundaries_df) +
   geom_point(aes(x = LD1, y = LD2, colour = species)) +
   scale_color_manual(values = c("darkorange","purple","cyan4")) +
   labs(title = "Decision boundaries as visualized by a bunch of points")
```

We can overlay the actual points from our actual training set onto this graph to see what regions they fall into and how our model will predict them.

```{r}
lda_viz <- predict(lda_train2) 
lda_viz <- cbind(train_penguins, lda_viz$x)
ggplot(lda_viz, aes(x = LD1, y = LD2, colour = species)) +
   geom_point() +
   geom_point(data = lda_boundaries_df,
              aes(x = LD1, y = LD2, colour = species),
              alpha = 0.22,
              size = 0.9) +
   theme_minimal() +
   scale_color_manual(values = c("darkorange","purple","cyan4"))
```

It looks like adding bill length helps separate the Adelie and Chinstrap penguins into two groups, although with the training data we can still see a few purple (Chinstrap) points that will be misclassified as orange (Adelie). Let's see how the model does on the validation set.

```{r}
lda_val2 <- predict(lda_train2, newdata = validation_penguins)

table(validation_penguins$species, lda_val2$class,
      dnn = c("observed", "predicted"))
```

Our second model seems to do really well! I'm pretty happy with this model. We predicted everything in the validation set. Let's see how it does on the testing dataset!


```{r}
lda_test2 <- predict(lda_train2, newdata = test_penguins)

table(test_penguins$species, lda_test2$class,
      dnn = c("observed", "predicted"))
```

We missed one classification, but that's to be expected. We will likely never do as well on the test set as we do on the training or validation sets. I encourage you to try including island as a predictor in these LDA models and look at the corresponding plots to see what happens when you include a factor variable as a predictor.
